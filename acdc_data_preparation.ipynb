{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe66930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient001', 'patient002', 'patient003', 'patient004', 'patient005', 'patient006', 'patient007', 'patient008', 'patient009', 'patient010', 'patient011', 'patient012', 'patient013', 'patient014', 'patient015', 'patient016', 'patient017', 'patient018', 'patient019', 'patient020', 'patient021', 'patient022', 'patient023', 'patient024', 'patient025', 'patient026', 'patient027', 'patient028', 'patient029', 'patient030', 'patient031', 'patient032', 'patient033', 'patient034', 'patient035', 'patient036', 'patient037', 'patient038', 'patient039', 'patient040', 'patient041', 'patient042', 'patient043', 'patient044', 'patient045', 'patient046', 'patient047', 'patient048', 'patient049', 'patient050', 'patient051', 'patient052', 'patient053', 'patient054', 'patient055', 'patient056', 'patient057', 'patient058', 'patient059', 'patient060', 'patient061', 'patient062', 'patient063', 'patient064', 'patient065', 'patient066', 'patient067', 'patient068', 'patient069', 'patient070', 'patient071', 'patient072', 'patient073', 'patient074', 'patient075', 'patient076', 'patient077', 'patient078', 'patient079', 'patient080', 'patient081', 'patient082', 'patient083', 'patient084', 'patient085', 'patient086', 'patient087', 'patient088', 'patient089', 'patient090', 'patient091', 'patient092', 'patient093', 'patient094', 'patient095', 'patient096', 'patient097', 'patient098', 'patient099', 'patient100']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('ACDC_dataset/training'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5defd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient101', 'patient102', 'patient103', 'patient104', 'patient105', 'patient106', 'patient107', 'patient108', 'patient109', 'patient110', 'patient111', 'patient112', 'patient113', 'patient114', 'patient115', 'patient116', 'patient117', 'patient118', 'patient119', 'patient120', 'patient121', 'patient122', 'patient123', 'patient124', 'patient125', 'patient126', 'patient127', 'patient128', 'patient129', 'patient130', 'patient131', 'patient132', 'patient133', 'patient134', 'patient135', 'patient136', 'patient137', 'patient138', 'patient139', 'patient140', 'patient141', 'patient142', 'patient143', 'patient144', 'patient145', 'patient146', 'patient147', 'patient148', 'patient149', 'patient150', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('ACDC_dataset/testing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6abf335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stratification done in 7.37s\n",
      "📁 Processing processed_acdc_dataset\\dataset\\train_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [01:54<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to processed_acdc_dataset\\pickled\\full_data\\train_set.pkl\n",
      "📁 Processing processed_acdc_dataset\\dataset\\validation_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:19<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to processed_acdc_dataset\\pickled\\full_data\\validation_set.pkl\n",
      "📁 Processing processed_acdc_dataset\\dataset\\test_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:12<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to processed_acdc_dataset\\pickled\\full_data\\test_set.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import errno\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import skimage.morphology as morph\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from scipy.fftpack import fftn, ifftn\n",
    "from skimage.feature import peak_local_max, canny\n",
    "from skimage.transform import hough_circle\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ================================================================\n",
    "#                    HELPER FUNCTIONS\n",
    "# ================================================================\n",
    "\n",
    "def heart_metrics(seg_3Dmap, voxel_size, classes=[3, 1, 2]):\n",
    "    \"\"\"Compute the volumes of each class in mL.\"\"\"\n",
    "    volumes = []\n",
    "    for c in classes:\n",
    "        seg_copy = np.copy(seg_3Dmap)\n",
    "        seg_copy[seg_copy != c] = 0\n",
    "        seg_copy = np.clip(seg_copy, 0, 1)\n",
    "        volume = seg_copy.sum() * np.prod(voxel_size) / 1000.\n",
    "        volumes.append(volume)\n",
    "    return volumes\n",
    "\n",
    "\n",
    "def ejection_fraction(ed_vol, es_vol):\n",
    "    \"\"\"Calculate ejection fraction (in %).\"\"\"\n",
    "    stroke_vol = ed_vol - es_vol\n",
    "    return (float(stroke_vol) / float(ed_vol)) * 100 if ed_vol != 0 else 0\n",
    "\n",
    "\n",
    "def myocardialmass(myocardvol):\n",
    "    \"\"\"Compute myocardial mass in grams (density = 1.05 g/mL).\"\"\"\n",
    "    return myocardvol * 1.05\n",
    "\n",
    "\n",
    "def imshow(*args, **kwargs):\n",
    "    \"\"\"Display multiple images in one row.\"\"\"\n",
    "    cmap = kwargs.get('cmap', 'gray')\n",
    "    title = kwargs.get('title', '')\n",
    "    if len(args) == 0:\n",
    "        raise ValueError(\"No images given to imshow\")\n",
    "\n",
    "    n = len(args)\n",
    "    if isinstance(cmap, str):\n",
    "        cmap = [cmap] * n\n",
    "    if isinstance(title, str):\n",
    "        title = [title] * n\n",
    "\n",
    "    plt.figure(figsize=(n * 5, 10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(args[i], cmap[i])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "#                    ROI PLOTTING & ANIMATION\n",
    "# ================================================================\n",
    "\n",
    "def plot_roi(data4D, roi_center, roi_radii):\n",
    "    \"\"\"Animate slices with ROI overlay.\"\"\"\n",
    "    x_c, y_c = roi_center\n",
    "    x_r, y_r = roi_radii\n",
    "    zslices, tframes = data4D.shape[2], data4D.shape[3]\n",
    "\n",
    "    for z in range(zslices):\n",
    "        slice_data = np.swapaxes(np.swapaxes(data4D[:, :, z, :], 0, 2), 1, 2)\n",
    "        roi_mask = np.zeros_like(slice_data[0])\n",
    "        roi_mask[x_c - x_r:x_c + x_r, y_c - y_r:y_c + y_r] = 1\n",
    "\n",
    "        slice_data[:, roi_mask > 0.5] *= 0.8\n",
    "        fig = plt.figure(1)\n",
    "        fig.canvas.manager.set_window_title(f'Slice {z}')\n",
    "\n",
    "        def animate_out(i):\n",
    "            im.set_data(slice_data[i])\n",
    "            return im\n",
    "\n",
    "        im = plt.imshow(slice_data[0], cmap='gray')\n",
    "        anim = animation.FuncAnimation(fig, animate_out, frames=tframes, interval=50)\n",
    "        anim.save(f'Cine_MRI_SAX_{z}.mp4', fps=50, extra_args=['-vcodec', 'libx264'])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "#                    FILE I/O UTILS\n",
    "# ================================================================\n",
    "\n",
    "def save_data(data, filename, out_path):\n",
    "    out_filename = os.path.join(out_path, filename)\n",
    "    with open(out_filename, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"✅ Saved to {out_filename}\")\n",
    "\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def copy(src, dest):\n",
    "    \"\"\"Safe copy of files/directories.\"\"\"\n",
    "    try:\n",
    "        shutil.copytree(src, dest, ignore=shutil.ignore_patterns())\n",
    "    except OSError as e:\n",
    "        if e.errno == errno.ENOTDIR:\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print(f\"⚠️ Directory not copied. Error: {e}\")\n",
    "\n",
    "\n",
    "def read_patient_cfg(path):\n",
    "    \"\"\"Reads Info.cfg and returns patient metadata.\"\"\"\n",
    "    info = {}\n",
    "    cfg_file = os.path.join(path, 'Info.cfg')\n",
    "    if not os.path.exists(cfg_file):\n",
    "        raise FileNotFoundError(f\"Missing Info.cfg in {path}\")\n",
    "    with open(cfg_file) as f:\n",
    "        for line in f:\n",
    "            key, val = line.rstrip().split(\": \")\n",
    "            info[key] = val\n",
    "    return info\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "#                    DATA GROUPING\n",
    "# ================================================================\n",
    "\n",
    "NORMAL, MINF, DCM, HCM, RV = 'NOR', 'MINF', 'DCM', 'HCM', 'RV'\n",
    "\n",
    "\n",
    "def group_patient_cases(src_path, out_path, force=False):\n",
    "    \"\"\"Group the ACDC patient data according to cardiac pathology.\"\"\"\n",
    "    walker = os.walk(src_path)\n",
    "    try:\n",
    "        cases = sorted(next(walker)[1])\n",
    "    except StopIteration:\n",
    "        raise FileNotFoundError(f\"No subdirectories found in {src_path}\")\n",
    "\n",
    "    dest_path = os.path.join(out_path, 'Patient_Groups')\n",
    "    if force and os.path.exists(dest_path):\n",
    "        shutil.rmtree(dest_path)\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "    for group in [NORMAL, MINF, DCM, HCM, RV]:\n",
    "        os.makedirs(os.path.join(dest_path, group), exist_ok=True)\n",
    "\n",
    "    for case in cases:\n",
    "        full_path = os.path.join(src_path, case)\n",
    "        try:\n",
    "            patient_group = read_patient_cfg(full_path)['Group']\n",
    "            copy(full_path, os.path.join(dest_path, patient_group, case))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {case}: {e}\")\n",
    "    return dest_path\n",
    "\n",
    "\n",
    "def generate_train_validate_test_set(src_path, dest_path):\n",
    "    \"\"\"Split grouped data into train/val/test (70/15/15).\"\"\"\n",
    "    SPLIT_TRAIN, SPLIT_VALID = 0.7, 0.15\n",
    "    dest_path = os.path.join(dest_path, 'dataset')\n",
    "    if os.path.exists(dest_path):\n",
    "        shutil.rmtree(dest_path)\n",
    "\n",
    "    os.makedirs(os.path.join(dest_path, 'train_set'))\n",
    "    os.makedirs(os.path.join(dest_path, 'validation_set'))\n",
    "    os.makedirs(os.path.join(dest_path, 'test_set'))\n",
    "\n",
    "    for group in next(os.walk(src_path))[1]:\n",
    "        group_path = os.path.join(src_path, group)\n",
    "        patients = next(os.walk(group_path))[1]\n",
    "        np.random.shuffle(patients)\n",
    "\n",
    "        n_train = int(SPLIT_TRAIN * len(patients))\n",
    "        n_valid = int((SPLIT_TRAIN + SPLIT_VALID) * len(patients))\n",
    "        splits = {\n",
    "            'train_set': patients[:n_train],\n",
    "            'validation_set': patients[n_train:n_valid],\n",
    "            'test_set': patients[n_valid:]\n",
    "        }\n",
    "\n",
    "        for split_name, patient_list in splits.items():\n",
    "            for p in patient_list:\n",
    "                copy(os.path.join(group_path, p),\n",
    "                     os.path.join(dest_path, split_name, p))\n",
    "    return dest_path\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "#                    ROI EXTRACTION\n",
    "# ================================================================\n",
    "\n",
    "def extract_roi_stddev(data4D, pixel_spacing, minradius_mm=15, maxradius_mm=45,\n",
    "                       kernel_width=5, center_margin=8, num_peaks=10, num_circles=20, radstep=2):\n",
    "    \"\"\"ROI extraction using StdDev + Hough Transform.\"\"\"\n",
    "    px, py, _, _ = pixel_spacing\n",
    "    minr, maxr = int(minradius_mm / px), int(maxradius_mm / py)\n",
    "\n",
    "    xsize, ysize, zslices, tframes = data4D.shape\n",
    "    xsurf = np.tile(np.arange(xsize), (ysize, 1)).T\n",
    "    ysurf = np.tile(np.arange(ysize), (xsize, 1))\n",
    "    lsurf = np.zeros((xsize, ysize))\n",
    "    centers, accums, radii = [], [], []\n",
    "\n",
    "    for z in range(zslices):\n",
    "        fh = np.std(np.array([data4D[:, :, z, t] for t in range(tframes)]), axis=0)\n",
    "        fh[fh < 0.1 * np.max(fh)] = 0\n",
    "        image = fh / np.max(fh)\n",
    "        edges = canny(image, sigma=3)\n",
    "        hough_radii = np.arange(minr, maxr, radstep)\n",
    "        hough_res = hough_circle(edges, hough_radii)\n",
    "\n",
    "        if not hough_res.any():\n",
    "            continue\n",
    "\n",
    "        for r, h in zip(hough_radii, hough_res):\n",
    "            peaks = peak_local_max(h, num_peaks=num_peaks)\n",
    "            centers.extend(peaks)\n",
    "            accums.extend(h[peaks[:, 0], peaks[:, 1]])\n",
    "            radii.extend([r] * len(peaks))\n",
    "\n",
    "        sorted_idx = np.argsort(accums)[::-1][:num_circles]\n",
    "        for idx in sorted_idx:\n",
    "            cx, cy = centers[idx]\n",
    "            brightness = accums[idx]\n",
    "            lsurf += brightness * np.exp(-((xsurf - cx) ** 2 + (ysurf - cy) ** 2) / kernel_width ** 2)\n",
    "\n",
    "    if lsurf.max() == 0:\n",
    "        return (0, 0), (0, 0)\n",
    "\n",
    "    lsurf /= lsurf.max()\n",
    "    roi_center = np.unravel_index(lsurf.argmax(), lsurf.shape)\n",
    "\n",
    "    roi_xr, roi_yr = 0, 0\n",
    "    for i in range(len(centers)):\n",
    "        dx, dy = abs(centers[i][0] - roi_center[0]), abs(centers[i][1] - roi_center[1])\n",
    "        if dx <= center_margin and dy <= center_margin:\n",
    "            roi_xr = max(roi_xr, radii[i] + dx)\n",
    "            roi_yr = max(roi_yr, radii[i] + dy)\n",
    "\n",
    "    return roi_center, (roi_xr, roi_yr) if roi_xr and roi_yr else None\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "#                    DATASET CLASS (TRAIN/VAL/TEST)\n",
    "# ================================================================\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, directory, subdir):\n",
    "        self.directory = directory\n",
    "        self.name = subdir\n",
    "        self.patient_data = {}\n",
    "\n",
    "    def _filename(self, file):\n",
    "        return os.path.join(self.directory, self.name, file)\n",
    "\n",
    "    def load_nii(self, img_path):\n",
    "        nimg = nib.load(self._filename(img_path))\n",
    "        return nimg.get_fdata(), nimg.affine, nimg.header\n",
    "\n",
    "    def read_patient_info_data(self):\n",
    "        path = self._filename('Info.cfg')\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                key, val = line.rstrip().split(\": \")\n",
    "                self.patient_data[key] = val\n",
    "\n",
    "    def read_patient_data(self, roi_detect=True):\n",
    "        pid = int(re.match(r\"patient(\\d{3})\", self.name).group(1))\n",
    "        self.read_patient_info_data()\n",
    "        ED = int(self.patient_data['ED'])\n",
    "        ES = int(self.patient_data['ES'])\n",
    "\n",
    "        ed_img, es_img = f\"patient{pid:03d}_frame{ED:02d}.nii.gz\", f\"patient{pid:03d}_frame{ES:02d}.nii.gz\"\n",
    "        ed, affine, hdr = self.load_nii(ed_img)\n",
    "        es, _, _ = self.load_nii(es_img)\n",
    "        self.patient_data['ED_VOL'], self.patient_data['ES_VOL'] = ed, es\n",
    "        self.patient_data['header'] = {'affine': affine, 'hdr': hdr}\n",
    "\n",
    "        ed_gt, _, _ = self.load_nii(f\"patient{pid:03d}_frame{ED:02d}_gt.nii.gz\")\n",
    "        es_gt, _, _ = self.load_nii(f\"patient{pid:03d}_frame{ES:02d}_gt.nii.gz\")\n",
    "        self.patient_data['ED_GT'], self.patient_data['ES_GT'] = ed_gt, es_gt\n",
    "\n",
    "        ed_lv, ed_rv, ed_myo = heart_metrics(ed_gt, hdr.get_zooms())\n",
    "        es_lv, es_rv, es_myo = heart_metrics(es_gt, hdr.get_zooms())\n",
    "        ef_lv, ef_rv = ejection_fraction(ed_lv, es_lv), ejection_fraction(ed_rv, es_rv)\n",
    "        self.patient_data['HP'] = {'EDV_LV': ed_lv, 'EDV_RV': ed_rv,\n",
    "                                   'ESV_LV': es_lv, 'ESV_RV': es_rv,\n",
    "                                   'EF_LV': ef_lv, 'EF_RV': ef_rv}\n",
    "\n",
    "        if roi_detect:\n",
    "            img4d, _, hdr = self.load_nii(f\"patient{pid:03d}_4d.nii.gz\")\n",
    "            self.patient_data['4D'] = img4d\n",
    "            c, r = extract_roi_stddev(img4d, hdr.get_zooms())\n",
    "            self.patient_data['roi_center'], self.patient_data['roi_radii'] = c, r\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "#                    MAIN EXECUTION\n",
    "# ================================================================\n",
    "\n",
    "def convert_nii_np(data_path, roi_detect):\n",
    "    patient_fulldata = OrderedDict()\n",
    "    print(f\"📁 Processing {data_path}\")\n",
    "    for patient in tqdm(sorted(next(os.walk(data_path))[1])):\n",
    "        dset = Dataset(data_path, patient)\n",
    "        dset.read_patient_data(roi_detect=roi_detect)\n",
    "        patient_fulldata[dset.name] = dset.patient_data\n",
    "    return patient_fulldata\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    complete_data_path = 'ACDC_dataset/training'\n",
    "    dest_path = 'processed_acdc_dataset'\n",
    "    group_path = os.path.join(dest_path, 'Patient_Groups')\n",
    "\n",
    "    train_dataset_path = os.path.join(dest_path, 'dataset', 'train_set')\n",
    "    val_dataset_path = os.path.join(dest_path, 'dataset', 'validation_set')\n",
    "    test_dataset_path = os.path.join(dest_path, 'dataset', 'test_set')\n",
    "    out_path_train = os.path.join(dest_path, 'pickled', 'full_data')\n",
    "\n",
    "    # === STRATIFY DATA ===\n",
    "    group_patient_cases(complete_data_path, dest_path)\n",
    "    generate_train_validate_test_set(group_path, dest_path)\n",
    "    print(f\"✅ Stratification done in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    # === ROI + PICKLE SAVE ===\n",
    "    os.makedirs(out_path_train, exist_ok=True)\n",
    "\n",
    "    for name, path in zip(['train_set', 'validation_set', 'test_set'],\n",
    "                          [train_dataset_path, val_dataset_path, test_dataset_path]):\n",
    "        data = convert_nii_np(path, roi_detect=True)\n",
    "        save_data(data, f\"{name}.pkl\", out_path_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
